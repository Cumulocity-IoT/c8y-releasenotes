---
weight: 40
title: Release 10.16.0
layout: bundle
---

Cumulocity IoT DataHub Release 10.16 includes the following improvements, limitations, and known issues:

### Support for new data model of measurements collection

Cumulocity IoT is equipped with a new data model underlying the measurements collection. The model represents measurements as a time-series in the Operational Store of Cumulocity IoT, which allows for a more efficient querying. The new data model is reflected in the offloading mechanism of Cumulocity IoT DataHub.

### Authentication against Azure data lake via Azure Active Directory

When using Azure Storage as data lake, Azure Active Directory credentials can be used for authentication.

### Configuration of offloading frequency

Per default an offloading run is executed once an hour, for each hour of the day. The offloading frequency can be configured more fine-granular by selecting at which hours of the day an offloading run will be executed.

### Minor UI enhancements

The offloading page has been enhanced with status information. It provides for an offloading configuration the status of the most recent offloading and compaction runs. Regarding the definition of an offloading configuration, the additional filter predicate step provides examples for filter predicates.

### Limitations

|<div style="width:250px">Description</div>
|:---
|Mixed usage of uppercase and lowercase characters for an attribute name as well as series names in the documents is not supported.|
|If the collection to be offloaded has JSON attributes consisting of more than 32,000 characters, its data cannot be offloaded. One specific case where this limitation applies is Cumulocity IoT's application builder, which stores its assets in the inventory collection when being used.|
|If the collection to be offloaded has more than 800 JSON attributes, its data cannot be offloaded. This limitation also includes nested JSON content, which will be expanded into columns during offloading. Therefore, measurements documents with more than 800 series/series value fragments are not supported.|
|If an attribute of a collection has varying types associated, the result table will contain a mixed type which may render query writing difficult or lead to problems with subsequent consumer applications.|
|Dremio has announced in its [Dremio 4.0 release notes](https://docs.dremio.com/release-notes/40-release-notes.html#deprecations) to deprecate some functionality on mixed types. In Dremio 13.2, GROUP BY or DISTINCT over mixed type expressions are no longer supported. It is necessary to cast these expressions to a simple type before using them in a GROUP BY or DISTINCT clause. Also mixed type expressions cannot be used in the SELECT clause, when retrieving data over ODBC or JDBC.|

### Known issues

|<div style="width:250px">Edition|Description|
|:---|:---|
|Cloud|Data lake configuration validation is broken in terms of wrong bucket names (AWS S3) and wrong account names (Azure Storage). When saving the settings with an invalid bucket/account name, DataHub fails to quickly detect the problem and will instead run a time-consuming check, which shows up as an ongoing save request in the UI. Eventually the request will fail in the UI with a timeout and the save request in the backend will fail as well. In such a case, please carefully check the bucket/account name and try saving again.|
|Cloud|For older deployments, if the *track changes* feature of Cumulocity IoT was not enabled, the readings in the Operational Store of Cumulocity IoT do not have a creation timestamp. If more than 4096 of those readings exist, the offloading will fail, even if *track changes* was enabled afterwards. In that case a Dremio administrator must run a SELECT * query without limit or filter over the collection in order to gather the correct schema, which is required for a successful offloading.|
|Edge|There are no retention policies in place that prevent the data lake contents from exceeding the hard disk limits.|
|Edge|TLS is not supported for ODBC and JDBC.|

### Planned

#### Breaking change in the offloading mechanism - the switch to new table format may introduce incompatibilities

In one of the future releases the offloading mechanism may leverage Apache Iceberg, which is an open table format supported by Dremio. Due to that adaptation, current offloading configurations may fail, e.g., because rarely used data types are incompatible.  